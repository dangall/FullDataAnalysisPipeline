{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "import requests #This allows you to get the HTML code\n",
    "from pattern import web #This allows you to explore the dom (i.e. the HTML structure)\n",
    "from bs4 import BeautifulSoup #This is an alternaticeve way of exploring the dom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes a column that will eventually be made numeric but is all dirty with badly made strings. It removes\n",
    "# the thousand-delimiters, it replaces the decimaldelimiters with periods, and removes any user-chosen \n",
    "# additional set of characters. But it leaves it as a string.\n",
    "def stripOffUselessCharacters(column,decimaldelimiter=\".\",thousanddelimeter=\",\",listofstringstoremove=[\"-\",\" \",\"%\"]):\n",
    "    toremoveregex = str(listofstringstoremove + [thousanddelimeter]).rstrip(\"]'\").lstrip(\"'[\").replace(\"', '\",\"|\")\n",
    "    cleancolumn = column.astype(str).str.replace(toremoveregex,\"\").str.replace(decimaldelimiter,\".\")\n",
    "    return cleancolumn\n",
    "\n",
    "# This function takes a website ending relating to a surname, fetches from http://forebears.io/\n",
    "# the prevalence of that surname, and turns that information into a table. The function then returns \n",
    "# a pandas dataframe where the first column is an ordered list of countries and the second is\n",
    "# the relative probability that the person with the surname comes from that country\n",
    "def getNationalitiesAndProbabilities(website_ending):\n",
    "    url = \"http://forebears.io/\" + website_ending #This is the URL whose HTML we want to scrape\n",
    "    # if the URL has parameters you can give it, we can do so with the dictionary,\n",
    "    # e.g. additional_requirements = {'key1': 'value1', 'key2': 'value2'} will make the URL:\n",
    "    # http://www.examplewebsite.com/subpage?key2=value2&key1=value1\n",
    "    additional_requirements = {}\n",
    "    r = requests.get(url, params=additional_requirements)\n",
    "    #The variable r contains the website HTML\n",
    "    # print r.url gives us the website address we just fetched\n",
    "    # print r.text gives the whole HTML for the page\n",
    "    \n",
    "    # Now we're going to go through the DOM of the website to pick out the table containing nationalities\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    # The table with nationalities and their frequencies is in the tag <data-tabset ...>\n",
    "    # Within that, each row is in a separate <tr> and the column headers are in <th>\n",
    "    frequencytable = soup.find_all('data-tabset')[0]\n",
    "    tablerows = frequencytable.find_all('tr')\n",
    "    columntitles = tablerows[0].find_all('th')\n",
    "    columntitles = [title.get_text() for title in columntitles]\n",
    "    \n",
    "    # Now we'll make a pandas dataframe containing the table information.\n",
    "    # First we'll populate a 2d python list with the data\n",
    "    tabledata = [[entry.get_text() for entry in row.find_all(\"td\")] for row in tablerows[1:]]\n",
    "    if [] in tabledata:\n",
    "        tabledata = tabledata[:tabledata.index([])]\n",
    "    tabledata = pd.DataFrame(tabledata, columns=columntitles)\n",
    "    \n",
    "    # The numeric data needs cleaning up, since it is in the form of strings at the moment\n",
    "    for colname in tabledata:\n",
    "        if colname!=\"Country\":\n",
    "            # we have a column that should be numeric. Begin by cleaning up the string\n",
    "            tabledata.loc[:,colname] = stripOffUselessCharacters(tabledata[colname])\n",
    "            # One column has the format 1:xxxx to indicate the prevalence of people with a given \n",
    "            # surname (one in xxxx has the name)\n",
    "            columnwithoutOneColon = tabledata[colname].str.replace(\"1:\",\"\")\n",
    "            if columnwithoutOneColon.equals(tabledata[colname]):\n",
    "                # we have a numeric column that isn't a ratio\n",
    "                tabledata.loc[:,colname] = pd.to_numeric(tabledata[colname]).astype(float)\n",
    "            else:\n",
    "                # we have the column which is a ratio\n",
    "                tabledata.loc[:,colname] = 1.0/pd.to_numeric(columnwithoutOneColon).astype(float)\n",
    "    \n",
    "    # Now we'll turn the Incidence column into a probability for the given surname to come from a certain country\n",
    "    tabledata.loc[:,\"Incidence\"] = tabledata[\"Incidence\"] / tabledata[\"Incidence\"].sum()\n",
    "    \n",
    "    return tabledata#[[\"Country\",\"Incidence\"]]\n",
    "\n",
    "# This function takes a surname, searches the website http://forebears.io , and takes the name on their database that\n",
    "# most closely matches the given surname. It then returns a list of the form [name,website link ending]\n",
    "# The second element of this list is intended to be fed to the function getNationalitiesAndProbabilities.\n",
    "def getProbableNameAndWebsite(surname):    \n",
    "    url = \"http://forebears.io/surnames\"\n",
    "    additional_requirements = {\"q\":surname}\n",
    "    r = requests.get(url, params=additional_requirements)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    listofcases = soup.find_all(\"div\", class_=\"bigItms\")[0]\n",
    "    if len(list(listofcases.children))>0:\n",
    "        mostprobablecase = listofcases.div.div.a\n",
    "        link = mostprobablecase[\"href\"]\n",
    "        name = mostprobablecase.get_text()\n",
    "    else:\n",
    "        name = surname\n",
    "        link = \"unknown\"\n",
    "    return [name,link]\n",
    "\n",
    "# This function takes a surname (as a string) and returns the country that this surname is likeliest to have come from\n",
    "def mostLikelyCountry(surname):\n",
    "    #print surname\n",
    "    #time.sleep(random.uniform(0.1,0.5)) # This is to be kind to the website and not overload it with requests all at once\n",
    "    nameandlink = getProbableNameAndWebsite(surname)\n",
    "    if nameandlink[1] != \"unknown\":\n",
    "        #time.sleep(random.uniform(0.3,0.7)) # again we're kind to the website\n",
    "        probablenationality = getNationalitiesAndProbabilities(nameandlink[1])[\"Country\"][0]\n",
    "    else:\n",
    "        probablenationality = \"Unknown\"\n",
    "    return probablenationality\n",
    "\n",
    "# This function takes a name in the format \"Surname, title and firstname\" and returns \"Surname\"\n",
    "def pickOutLastName(fullname):\n",
    "    surname = fullname\n",
    "    # first we remove all text after the comma\n",
    "    if \",\" in surname:\n",
    "        surname = surname[:surname.index(\",\")]\n",
    "    # then we remove all text after a possible hyphen \n",
    "    # (in case of hyphenated names, sometimes found in multicultural children)\n",
    "    if \"-\" in surname:\n",
    "        surname = surname[:surname.index(\"-\")]\n",
    "    return surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Here I specify which data files need reading in\n",
    "datafilenames = [\"cleantrain.csv\",\"cleantest.csv\"]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "alldataframes = [pd.read_csv(filename) for filename in datafilenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# ONLY RUN THIS IF YOU HAVEN'T RUN IT ALREADY! IT TAKES QUITE A LONG TIME DUE TO THE PAUSE TIME WE GIVE TO THE WEBSITE\n",
    "# We now want to add columns that can be inferred from the other columns (feature engineering for feature selection)\n",
    "\n",
    "ii=0\n",
    "namecolumn = alldataframes[ii][\"Name\"].apply(pickOutLastName)\n",
    "\n",
    "# Now we'll get the countries from the website http://forebears.io/ and plug them into a new column\n",
    "countrycolumn = namecolumn.apply(mostLikelyCountry)\n",
    "alldataframes[ii][\"Nationality\"] = countrycolumn\n",
    "ii=1\n",
    "namecolumn = alldataframes[ii][\"Name\"].apply(pickOutLastName)\n",
    "countrycolumn = namecolumn.apply(mostLikelyCountry)\n",
    "alldataframes[ii][\"Nationality\"] = countrycolumn\n",
    "#================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# We are finished cleaning the data. We'll now output the clean data to a new csv file\n",
    "# Here I specify the names of the clean-data files\n",
    "outputfilenames = [\"natinalitycleantrain.csv\",\"natinalitycleantest.csv\"]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    alldataframes[ii].to_csv(outputfilenames[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
