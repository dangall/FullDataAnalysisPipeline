{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Here I specify which data files need reading in\n",
    "datafilenames = [\"train.csv\",\"test.csv\"]\n",
    "#================================================================================================\n",
    "\n",
    "alldataframes = [pd.read_csv(filename) for filename in datafilenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for detecting dirty data and cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes a column and does a tally of the different types of entries (int, float, string, etc.)\n",
    "# It then spits out a tuple of the different types and their relative frequencies in the column\n",
    "def ratiosOfDifferentTypes(column):\n",
    "    uniquerows = column.dropna().drop_duplicates()\n",
    "    uniquerows = uniquerows.sample(n=min(1000,uniquerows.size))\n",
    "    types = [type(entry) for entry in uniquerows]\n",
    "    differenttypes = list(set(types))\n",
    "    tally = [types.count(giventype) for giventype in differenttypes]\n",
    "    total = sum(tally)*1.\n",
    "    frequencies = [tallyelement / total for tallyelement in tally]\n",
    "    return (differenttypes,frequencies)\n",
    "\n",
    "# This function takes a column and decides which type its entries are meant to be like.\n",
    "# It returns the type. If the entries are so mixed that it can't decide, it returns object.\n",
    "def decideType(column):\n",
    "    typesandratios = ratiosOfDifferentTypes(column)\n",
    "    if max(typesandratios[1]) >= 0.8:\n",
    "        # all the rows should probably be of the same type and some have been inputted incorrectly\n",
    "        correcttype = typesandratios[0][typesandratios[1].index(max(typesandratios[1]))]\n",
    "    else:\n",
    "        # the rows have a very mixed type and it's not very clear what the correct type is\n",
    "        correcttype = object\n",
    "    return correcttype\n",
    "\n",
    "# This function goes through all columns in the dataframe and returns the name of the columns that are dirty,\n",
    "# i.e. that have mixed types of entries.\n",
    "def findMixedTypes(dataframe):\n",
    "    return [col for col in dataframe if len(ratiosOfDifferentTypes(dataframe[col])[1])>1]\n",
    "\n",
    "# This function takes a dataframe and for each column says whether it's clean or dirty. If it's dirty,\n",
    "# it tries to decide which type it should be.\n",
    "def analyzeColumnTypes(dataframe):\n",
    "    mixedtypecolumns = findMixedTypes(dataframe)\n",
    "    if mixedtypecolumns==[]:\n",
    "        print \"All columns have a single type; they are 'clean'. (They may be incorrect though, or have NaNs).\"\n",
    "    else:\n",
    "        print \"The columns\",mixedtypecolumns,\"have mixed types:\\n\"\n",
    "        correcttypes = [(colname,decideType(dataframe[colname])) for colname in mixedtypecolumns]\n",
    "        for typ in correcttypes:\n",
    "            if typ[1]==object:\n",
    "                print \" - \\'\" + typ[0] + \"\\'\" + \" is so mixed it's hard to tell the right type\"\n",
    "            else:\n",
    "                print \" - \\'\" + typ[0] + \"\\'\" + \" should be \" + \"\\'\" + typ[1].__name__ + \"\\'\"\n",
    "        print \"\\nAll other columns have a single type; they are 'clean'. (They may be incorrect though, or have NaNs).\"\n",
    "    return mixedtypecolumns\n",
    "\n",
    "def findCleanStringTypes(dataframe):\n",
    "    return [col for col in dataframe if ratiosOfDifferentTypes(dataframe[col])[0]==[str]]\n",
    "\n",
    "def outlineNaNs(dataframe):\n",
    "    totalnumberofNaNs = pd.isnull(dataframe).sum()\n",
    "    percentageofNaNs = totalnumberofNaNs[totalnumberofNaNs > 0].astype(np.float64) / dataframe.shape[0]\n",
    "    if len(percentageofNaNs)>0:\n",
    "        print \"Here are the columns with NaNs:\"\n",
    "        for kk in range(len(percentageofNaNs)):\n",
    "            print \" - \\'\" + percentageofNaNs.index[kk] + \"\\' has percentage of NaNs: \\t\" + str(percentageofNaNs[kk]) + \" %\"\n",
    "        print \"\\nNo other columns have NaNs.\"\n",
    "    else:\n",
    "        print \"No columns have NaNs.\"\n",
    "\n",
    "def stripStartEndSpaces(listofdataframes):\n",
    "    outputdataframes = listofdataframes\n",
    "    for ii in range(len(listofdataframes)):\n",
    "        cleanstringcolumns = findCleanStringTypes(listofdataframes[ii])\n",
    "        for colname in cleanstringcolumns:\n",
    "            outputdataframes[ii].loc[:,colname] = listofdataframes[ii][colname].str.strip()\n",
    "    return outputdataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing data types\n",
    "\n",
    "Now we're going to print out which columns have dirty entries, i.e. have mixed types, and we'll try and guess what those entries should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mixedtypesindataframes = range(len(alldataframes))\n",
    "for ii in range(len(alldataframes)):\n",
    "    print \"ANALYZING DATAFRAME FROM \" + datafilenames[ii] + \":\"\n",
    "    print \"====================================================\"\n",
    "    mixedtypesindataframes[ii] =  analyzeColumnTypes(alldataframes[ii])\n",
    "    print \"-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\"\n",
    "    outlineNaNs(alldataframes[ii])\n",
    "    print \"====================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding strange / outlier data\n",
    "Let's look at the UNIQUE values. We'll go through the dirty entries, as well as the purely-string entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii in range(len(alldataframes)):\n",
    "    print \"ANALYZING DATAFRAME FROM \" + datafilenames[ii] + \":\"\n",
    "    print \"----------------------------------------------------\"\n",
    "    cleanstringcolumns = findCleanStringTypes(alldataframes[ii])\n",
    "    for col in mixedtypesindataframes[ii] + cleanstringcolumns:\n",
    "        print \"Column '\" + col + \"' has the following unique entries:\\n\"\n",
    "        print np.sort(alldataframes[ii][col].unique())\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing bad datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Now we need to remove those rows of data that are missing critical information, i.e. remove \n",
    "# those rows that have a NaN for something very important.\n",
    "criticalcolumns = [\n",
    "    [\"Survived\",\"Pclass\"],\n",
    "    []\n",
    "]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    alldataframes[ii] = alldataframes[ii].dropna(subset = criticalcolumns[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# In the remaining columns there may be some NaNs, which should be replaced with some appropriate value.\n",
    "# The variable whattodowithnans has the structure of a dictionary for every dataframe, i.e. [{\"colname\": valueforNaN,...},...]\n",
    "\n",
    "whattodowithnans = [\n",
    "    {\"Survived\": -1, \"Pclass\": -1,\"Name\": \"Unknown name\", \"Sex\": \"Unspecified\", \n",
    "     \"Age\": alldataframes[0][\"Age\"].dropna().mean(), \"Ticket\": \"XXXXXX\", \n",
    "     \"Cabin\": \"XXX\", \"Embarked\": \"X\"},\n",
    "    {\"Pclass\": -1,\"Name\": \"Unknown name\", \"Sex\": \"Unspecified\", \n",
    "     \"Age\": alldataframes[0][\"Age\"].dropna().mean(), \"Ticket\": \"XXXXXX\", \n",
    "     \"Cabin\": \"XXX\", \"Embarked\": \"X\"}\n",
    "]\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    alldataframes[ii] = alldataframes[ii].fillna(whattodowithnans[ii])\n",
    "\n",
    "alldataframes = stripStartEndSpaces(alldataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Sometimes we want to fill the NaNs differently depending on the values of the other columns. Here we may do this\n",
    "\n",
    "# Here we replace the ticket fare depending on the Pclass. We take the average Fare for a given Pclass\n",
    "conditioncolumn = \"Pclass\"\n",
    "shouldequal = [1,2,3]\n",
    "toreplacecolumn = \"Fare\"\n",
    "\n",
    "averageanswers = alldataframes[0].groupby(conditioncolumn)[toreplacecolumn].mean()\n",
    "ii=0\n",
    "for val in shouldequal:\n",
    "    colrowstoreplacenans = alldataframes[ii][toreplacecolumn][alldataframes[ii][conditioncolumn]==val]\n",
    "    indices = colrowstoreplacenans.index\n",
    "    alldataframes[ii].loc[indices,toreplacecolumn] = colrowstoreplacenans.fillna(averageanswers[val])\n",
    "ii=1\n",
    "for val in shouldequal:\n",
    "    colrowstoreplacenans = alldataframes[ii][toreplacecolumn][alldataframes[ii][conditioncolumn]==val]\n",
    "    indices = colrowstoreplacenans.index\n",
    "    alldataframes[ii].loc[indices,toreplacecolumn] = colrowstoreplacenans.fillna(averageanswers[val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# After having run the previous cells you know which columns are clean and dirty.\n",
    "# Look at the unique values of the dirty ones, given above. Use this information to clean them up\n",
    "\n",
    "# In each dataframe, there are certain dirty columns that should be numeric\n",
    "columnsthatshouldbenumeric = [\n",
    "    [\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Fare\"],\n",
    "    [\"Pclass\",\"Age\",\"SibSp\",\"Fare\"]\n",
    "]\n",
    "# In each column there will be some conventions on how the bad things are written out.\n",
    "# For each column that should be numeric, we specify a tuple with the info\n",
    "# (decimaldelimiter (a string),thousanddelimeter (a string), listofstringstoremove (a list))\n",
    "structureofeachcolumn = [\n",
    "    [\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"]),\n",
    "        (\".\",\",\",[\"-\",\" \"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"])\n",
    "    ],\n",
    "    [\n",
    "        (\".\",\",\",[\"-\",\" \"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"]),\n",
    "        (\".\",\",\",[\"-\",\" \",\"%\"])\n",
    "    ]\n",
    "]\n",
    "\n",
    "columnsthatshouldbestrings = [\n",
    "    [\"Name\",\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\"],\n",
    "    [\"Name\",\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\"]\n",
    "]\n",
    "\n",
    "columnsthatshouldbedatetimes = [\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "# This function takes a column that should be numeric but is all dirty with badly made strings. It removes\n",
    "# the thousand-delimiters, it replaces the decimaldelimiters with periods, and removes any user-chosen \n",
    "# additional set of characters\n",
    "def turnToNumeric(column,decimaldelimiter=\".\",thousanddelimeter=\",\",listofstringstoremove=[\"-\",\" \",\"%\"]):\n",
    "    toremoveregex = str(listofstringstoremove + [thousanddelimeter]).rstrip(\"]'\").lstrip(\"'[\").replace(\"', '\",\"|\")\n",
    "    numericcolumn = pd.to_numeric(column.astype(str).str.replace(toremoveregex,\"\").str.replace(decimaldelimiter,\".\"))\n",
    "    return numericcolumn\n",
    "\n",
    "# This function takes a dirty column that should all be strings and turns it into such\n",
    "def turnToString(column):\n",
    "    return column.astype(str)\n",
    "\n",
    "def turnToDate(column):\n",
    "    return pd.to_datetime(column,dayfirst=True)\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    for (jj,coltofix) in enumerate(columnsthatshouldbestrings[ii]):\n",
    "        alldataframes[ii].loc[:,coltofix] = turnToString(alldataframes[ii][coltofix])\n",
    "    \n",
    "    for (jj,coltofix) in enumerate(columnsthatshouldbedatetimes[ii]):\n",
    "        alldataframes[ii].loc[:,coltofix] = turnToDate(alldataframes[ii][coltofix])\n",
    "        \n",
    "    for (jj,coltofix) in enumerate(columnsthatshouldbenumeric[ii]):\n",
    "        alldataframes[ii].loc[:,coltofix] = turnToNumeric(alldataframes[ii][coltofix],\n",
    "                                                    structureofeachcolumn[ii][jj][0],\n",
    "                                                    structureofeachcolumn[ii][jj][1],\n",
    "                                                    structureofeachcolumn[ii][jj][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing fake / duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Some times there are duplicate entries. Some columns provide unique identifiers for the rows,\n",
    "# for identifying unique entries (e.g. email address, full name, etc.). If there is no identifier\n",
    "# column, we just plug the empty list [] into identifiercolumns.\n",
    "\n",
    "identifiercolumns = [\n",
    "    [\"PassengerId\"],\n",
    "    [\"PassengerId\"]\n",
    "]\n",
    "\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    if identifiercolumns[ii] != []:\n",
    "        alldataframes[ii] = alldataframes[ii].drop_duplicates(identifiercolumns[ii])\n",
    "    else:\n",
    "        alldataframes[ii] = alldataframes[ii].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Some data in our database can be fake data (e.g. data generating from testing whether the database works).\n",
    "# This should be identified using .unique() on each column, above, and thrown away.\n",
    "#  - It could be numeric data that is too big, too small, or at some impossible value.\n",
    "#  - It could be impossible datetime stamps\n",
    "#  - It could be strings that don't make sense; they might contain, end, or start with something bad.\n",
    "#    They might be missing some parts in the string, or be too long or too short.\n",
    "\n",
    "# NUMERIC DATA (these are the conditions the numbers should satisfy)\n",
    "for ii in [0,1]:\n",
    "    if ii==0:\n",
    "        alldataframes[ii] = alldataframes[ii][(alldataframes[ii][\"Survived\"]==-1) | (alldataframes[ii][\"Survived\"]==0) | \n",
    "                                    (alldataframes[ii][\"Survived\"]==1)]\n",
    "    alldataframes[ii] = alldataframes[ii][(alldataframes[ii][\"Pclass\"]==-1) | (alldataframes[ii][\"Pclass\"]==1) | \n",
    "                                    (alldataframes[ii][\"Pclass\"]==2) | (alldataframes[ii][\"Pclass\"]==3)]\n",
    "    alldataframes[ii] = alldataframes[ii][(-0.1 < alldataframes[ii][\"Age\"]) & (alldataframes[ii][\"Age\"] < 120)]\n",
    "    alldataframes[ii] = alldataframes[ii][(0.0 < alldataframes[ii][\"Fare\"])]\n",
    "\n",
    "# TIMESTAMP DATA (these are the conditions the timestamps should satisfy)\n",
    "\n",
    "# STRING DATA (these are the conditions the strings should satisfy)\n",
    "for ii in [0,1]:\n",
    "    alldataframes[ii] = alldataframes[ii][(alldataframes[ii][\"Name\"].str.contains(\"Pinko\") & alldataframes[ii][\"Name\"].str.contains(\"Pallino\"))==False]\n",
    "    alldataframes[ii] = alldataframes[ii][(alldataframes[ii][\"Ticket\"]!=\"False\") & (alldataframes[ii][\"Ticket\"]!=\"false\")]\n",
    "    alldataframes[ii] = alldataframes[ii][(alldataframes[ii][\"Cabin\"]!=\"Deck\") & (alldataframes[ii][\"Cabin\"]!=\"deck\")]\n",
    "\n",
    "#================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polishing up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Now the data is essentially clean. It is likely to still have outliers, and things that don't make sense.\n",
    "# We'll polish it up even further.\n",
    "\n",
    "alldataframes[0] = alldataframes[0].drop(\"PassengerId\", 1)\n",
    "alldataframes[1] = alldataframes[1].drop(\"PassengerId\", 1)\n",
    "\n",
    "# This turns a cabin number into simply the first letter of the cabin, i.e. the generic area on the ship\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    alldataframes[ii].loc[:,\"Cabin\"] = alldataframes[ii][\"Cabin\"].apply(lambda x: x[0])\n",
    "\n",
    "#================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very basic exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# Now we'll do some exploratory analysis. This will help find outliers and strange things in the data, \n",
    "# and in turn help us clean it further. Proper regressions etc. are in a separate notebook.\n",
    "\n",
    "# READ THE OUTPUTS CAREFULLY!\n",
    "# First we'll do a general \"describe\" on the data. This is useful to find outliers in min & max, etc.\n",
    "alldataframes[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll do a histogram of each column, to get an idea of the distribution of its values\n",
    "alldataframes[0]._get_numeric_data().hist(bins=30, figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldataframes[0].groupby(\"Pclass\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldataframes[0].groupby(\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print alldataframes[0][alldataframes[0][\"Age\"] < 13][\"Survived\"].mean()\n",
    "print alldataframes[0][alldataframes[0][\"Age\"] >= 13][\"Survived\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean data in new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# We are finished cleaning the data. We'll now output the clean data to a new csv file\n",
    "# Here I specify the names of the clean-data files\n",
    "outputfilenames = [\"cleantrain.csv\",\"cleantest.csv\"]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "for ii in range(len(alldataframes)):\n",
    "    alldataframes[ii].to_csv(outputfilenames[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
