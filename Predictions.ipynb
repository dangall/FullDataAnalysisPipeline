{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniele\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import all the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomTreesEmbedding\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# USER INPUT!\n",
    "# Here I specify which data files need reading in\n",
    "datafilenames = [\"natinalitycleantrain.csv\"]\n",
    "# For each data file, we write which column contains the information we want to predict\n",
    "topredictcolumn= [\"Survived\"]\n",
    "#================================================================================================\n",
    "\n",
    "# FROM HERE ON IT'S AUTOMATIC\n",
    "\n",
    "# Sometiems the index column gets saved automatically as a new column when writingto csv. \n",
    "# We'll remove these as they serve no purpose.\n",
    "alldataframes = [pd.read_csv(filename) for filename in datafilenames]\n",
    "\n",
    "for (pos,frame) in enumerate(alldataframes):\n",
    "    toadd = frame\n",
    "    if \"Unnamed: 0\" in frame:\n",
    "        toadd = toadd.drop(\"Unnamed: 0\",axis=1)\n",
    "    if \"Unnamed: 0.1\" in frame:\n",
    "        toadd = toadd.drop(\"Unnamed: 0.1\",axis=1)\n",
    "    alldataframes[pos] = toadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular problem is a classification problem; we are not trying to obtain the value of some parameter, e.g. how much a customer will spend in the future.\n",
    "\n",
    "However, it might still be useful to try and predict such parameters, e.g. the age of passengers. This might improve survival predictions, but regarless it is a good exercise to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thedataframe = deepcopy(alldataframes[0])\n",
    "datacateg = {'Survived': \"QA\", 'Pclass': \"O\", 'Name': \"C\", 'Sex': \"C\",\n",
    "             'Age': \"QA\", 'SibSp': \"QA\", 'Parch': \"QA\", 'Ticket': \"C\", \n",
    "             'Fare': \"QA\", 'Cabin': \"C\", 'Embarked': \"C\", 'Nationality': \"C\"}\n",
    "uselesscols = [\"Name\", \"Ticket\"]\n",
    "dictofunknown = {\"Survived\": -1, \"Pclass\": -1,\"Name\": \"Unknown name\", \"Sex\": \"Unspecified\", \n",
    "                 \"Ticket\": \"XXXXXX\", \"Cabin\": \"X\", \"Embarked\": \"X\", \n",
    "                 \"Age\": alldataframes[0][\"Age\"][abs(alldataframes[0][\"Age\"] - alldataframes[0][\"Age\"].mean()) < 0.2].unique()[0], \n",
    "                 \"Nationality\": \"Unknown\"}\n",
    "orderings = [\n",
    "    {\"Pclass\": [1,2,3]} #NB only need to include columns that are not numeric!\n",
    "]\n",
    "dictofordinals = {\"Pclass\": [1,2,3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "### Functions for making predictions based on given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns the intercept, parameters, Rscore (where 1.0 is perfect prediction, and percentage of cases\n",
    "# correctly predicted. The final two are given as tuple (score, errorbarsonscore). X is the data we use \n",
    "# to predict y; predictionmethod is the name of our model, e.g. linear_model.LinearRegression; rounddecimals\n",
    "# equals the number of decimals allowed for our prediction, e.g. when predicting integers rounddecimals=0; \n",
    "# num_iterations is the number of times we run the function to obtain the parameters, and kwargs are any \n",
    "# additional optional parameters we want to pass on to predictionmethod.\n",
    "def getParametersAndPredictionScore(inputX, inputy, predictionmethod, rounddecimals=\"none\", num_iterations=1, \n",
    "                                    classifier=False, transform=\"none\",**kwargs):\n",
    "    (X, y) = (inputX, inputy)\n",
    "    if transform!=\"none\":\n",
    "        (X, y) = (transformData(inputX, transform=transform), transformData(inputy, transform=transform))\n",
    "    if classifier==True:\n",
    "        predictedpercent = []\n",
    "    parameters = []\n",
    "    intercept = []\n",
    "    score = []\n",
    "    for iter in range(num_iterations):\n",
    "        kfoldindices = KFold(len(y),n_folds=5,shuffle=True)\n",
    "        for trainindex, testindex in kfoldindices:\n",
    "            Xtrain, Xtest = X[trainindex], X[testindex]\n",
    "            if transform==\"scale\":\n",
    "                (Xtrain, Xtest) = scaleData(Xtrain, Xtest)\n",
    "            ytrain, ytest = y[trainindex], y[testindex]\n",
    "            predictor = predictionmethod(**kwargs)\n",
    "            predictor.fit(Xtrain,ytrain)\n",
    "            predictedy = predictor.predict(Xtest)\n",
    "            if rounddecimals!=\"none\":\n",
    "                predictedy = np.round(predictedy, decimals=rounddecimals)\n",
    "            if classifier==True:\n",
    "                predictedpercent.append((predictedy==ytest).mean())\n",
    "            parameters.append(predictor.coef_)\n",
    "            intercept.append(predictor.intercept_)\n",
    "            score.append(predictor.score(Xtest,ytest))\n",
    "    score = (np.mean(score),np.std(score))\n",
    "    parameters = np.mean(parameters, axis=0)\n",
    "    intercept = np.mean(intercept)\n",
    "    if classifier==True:\n",
    "        predictedpercent = (np.mean(predictedpercent), np.std(predictedpercent))\n",
    "        toreturn = (intercept, parameters, score, predictedpercent)\n",
    "    else:\n",
    "        toreturn = (intercept, parameters, score)\n",
    "    return toreturn\n",
    "\n",
    "def getPredictionPercentage(inputX, inputy, predictionmethod, rounddecimals=\"none\", num_iterations=1, \n",
    "                                    classifier=False, transform=\"none\",**kwargs):\n",
    "    (X, y) = (inputX, inputy)\n",
    "    if transform!=\"none\":\n",
    "        (X, y) = (transformData(inputX, transform=transform), transformData(inputy, transform=transform))\n",
    "    if classifier==True:\n",
    "        predictedpercent = []\n",
    "    score = []\n",
    "    for iter in range(num_iterations):\n",
    "        kfoldindices = KFold(len(y),n_folds=5,shuffle=True)\n",
    "        for trainindex, testindex in kfoldindices:\n",
    "            Xtrain, Xtest = X[trainindex], X[testindex]\n",
    "            if transform==\"scale\":\n",
    "                (Xtrain, Xtest) = scaleData(Xtrain, Xtest)\n",
    "            ytrain, ytest = y[trainindex], y[testindex]\n",
    "            predictor = predictionmethod(**kwargs)\n",
    "            predictor.fit(Xtrain,ytrain)\n",
    "            predictedy = predictor.predict(Xtest)\n",
    "            if rounddecimals!=\"none\":\n",
    "                predictedy = np.round(predictedy, decimals=rounddecimals)\n",
    "            if classifier==True:\n",
    "                predictedpercent.append((predictedy==ytest).mean())\n",
    "            score.append(predictor.score(Xtest,ytest))\n",
    "    score = (np.mean(score),np.std(score))\n",
    "    if classifier==True:\n",
    "        predictedpercent = (np.mean(predictedpercent), np.std(predictedpercent))\n",
    "        toreturn = (score, predictedpercent)\n",
    "    else:\n",
    "        toreturn = (score)\n",
    "    return toreturn\n",
    "\n",
    "# Returns the prediction accuracy (+/- errorbars on the accuracy) when classifying \"at random\", i.e. \n",
    "# by always predicting the same thing for all the elements. We try by predicting the mean, or the \n",
    "# median, or the most frequently occurring element\n",
    "def randomPrediction(y, rounddecimals=\"none\", num_iterations=1):\n",
    "    meanscore = []\n",
    "    medianscore = []\n",
    "    mostfrequentscore = []\n",
    "    for iter in range(num_iterations):\n",
    "        kfoldindices = KFold(len(y),n_folds=5,shuffle=True)\n",
    "        for trainindex, testindex in kfoldindices:\n",
    "            ytrain, ytest = y[trainindex], y[testindex]\n",
    "            mean = np.mean(ytrain)\n",
    "            median = np.median(ytrain)\n",
    "            mostfrequent = max(set(list(y)), key=list(ytrain).count)\n",
    "            if rounddecimals!=\"none\":\n",
    "                mean = np.round(mean, decimals=rounddecimals)\n",
    "            meanscore.append((ytest==mean).mean())\n",
    "            medianscore.append((ytest==median).mean())\n",
    "            mostfrequentscore.append((ytest==mostfrequent).mean())\n",
    "    meanscore = (np.mean(meanscore), np.std(meanscore))\n",
    "    medianscore = (np.mean(medianscore), np.std(medianscore))\n",
    "    mostfrequentscore = (np.mean(mostfrequentscore), np.std(mostfrequentscore))\n",
    "    allscores = [meanscore[0], medianscore[0], mostfrequentscore[0]]\n",
    "    return [meanscore, medianscore, mostfrequentscore][allscores.index(max(allscores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformData(inputdata, transform=\"log\"):\n",
    "    outputdata = deepcopy(inputdata)\n",
    "    if transform==\"sqrt\":\n",
    "        outputdata[outputdata>0] = np.sqrt(outputdata[outputdata>0])\n",
    "        outputdata[outputdata<0] = -np.sqrt(-outputdata[outputdata<0])\n",
    "    if transform==\"log\":\n",
    "        outputdata[outputdata>1] = np.log(outputdata[outputdata>1])+1\n",
    "        outputdata[outputdata<-1] = -np.log(-outputdata[outputdata<-1])-1\n",
    "    return outputdata\n",
    "\n",
    "def inverseTransformData(inputdata, transform=\"log\"):\n",
    "    outputdata = deepcopy(inputdata)\n",
    "    if transform==\"sqrt\":\n",
    "        outputdata[outputdata>0] = outputdata[outputdata>0]**2\n",
    "        outputdata[outputdata<0] = -((-outputdata[outputdata<0])**2)\n",
    "    if transform==\"log\":\n",
    "        outputdata[outputdata>1] = np.e**(outputdata[outputdata>1]-1)\n",
    "        outputdata[outputdata<-1] = -(np.e**(-outputdata[outputdata<-1] -1))\n",
    "    return outputdata\n",
    "\n",
    "def scaleData(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Need to fit only on training data, not evaluation data!\n",
    "    scaledX_train = scaler.transform(X_train)\n",
    "    scaledX_test = scaler.transform(X_test)  # we apply same transformation to test data\n",
    "    return (scaledX_train, scaledX_test)\n",
    "\n",
    "#This function removes those rows in \"inputdataframe\" where data was generated automatially \n",
    "# in the column \"columnofinterest\". \"dictofunknown\" contains the information on what data was\n",
    "# generated automatically.\n",
    "def removeAutogeneratedData(inputdataframe, columnsofinterest, dictofunknown):\n",
    "    if type(columnsofinterest)==str:\n",
    "        allcolumnsofinterest = [columnsofinterest]\n",
    "    else:\n",
    "        allcolumnsofinterest = columnsofinterest\n",
    "    outputdataframe = deepcopy(inputdataframe)\n",
    "    for col in allcolumnsofinterest:\n",
    "        if col in dictofunknown:\n",
    "            outputdataframe = outputdataframe[outputdataframe[col]!= dictofunknown[col]]\n",
    "    return outputdataframe\n",
    "\n",
    "# Helper function that takes a model, e.g. linear_model.LinearRegression,\n",
    "# and returns the name of that model, e.g. \"LinearRegression\"\n",
    "def getModelName(currentmodel):\n",
    "    modelname = str(currentmodel)\n",
    "    modelname = modelname[modelname.rfind(\".\")+1:]\n",
    "    extracharacters = re.search(\"[^a-zA-z]\",modelname)\n",
    "    if type(extracharacters)!=type(None):\n",
    "        modelname = modelname[:extracharacters.start()]\n",
    "    return modelname\n",
    "\n",
    "# Returns the same dataframe but where the ordinal data has been replaced with numbers, and hence can\n",
    "# be ordered correctly. When plotting it will be necessary to replace the numbers back to their\n",
    "# original categorical labels.\n",
    "def orderOrdinalData(thedataframe, dictofordinals):\n",
    "    orderedordinalsframe = deepcopy(thedataframe)\n",
    "    # We need to turn the dictionary dictofordinals into a form good for using pandas .replace(dict)\n",
    "    neworderings = deepcopy(dictofordinals)\n",
    "    for key in neworderings:\n",
    "        # Pandas complains if we have intersections between the keys and the values\n",
    "        intersectionlength = len(set(neworderings[key]).intersection(set(range(1,len(neworderings[key])+1))))\n",
    "        if intersectionlength==0:\n",
    "            # We can just replace the entries with 0,1,2,...,num_categories-1\n",
    "            neworderings[key] = dict([(el,ii+1) for ii,el in enumerate(neworderings[key])])\n",
    "        else:\n",
    "            # We replace with '0',1','2',.... instead. We will later turn the strings into ints.\n",
    "            neworderings[key] = dict([(el,str(ii+1)) for ii,el in enumerate(neworderings[key])])\n",
    "    # Now we'll replace the ordinal columns in the dataframes with numeric labels\n",
    "    if dictofordinals!={}:\n",
    "        orderedordinalsframe.replace(neworderings, inplace=True)\n",
    "        for key in neworderings.keys():\n",
    "            orderedordinalsframe.loc[:,key] = pd.to_numeric(orderedordinalsframe[key])\n",
    "    return (orderedordinalsframe, neworderings)\n",
    "\n",
    "def makeAllColumnsNumeric(thedataframe, dictofordinals, datacateg):\n",
    "    ordereddataframe = orderOrdinalData(thedataframe, dictofordinals)[0]\n",
    "    replacementdict = {}\n",
    "    for col in ordereddataframe:\n",
    "        if datacateg[col]==\"C\":\n",
    "            allcategories = sorted(ordereddataframe[col].unique())\n",
    "            replacementdict[col] = dict([(cat,ii) for ii, cat in enumerate(allcategories)])\n",
    "    ordereddataframe.replace(replacementdict, inplace=True)\n",
    "    return (ordereddataframe, replacementdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement all linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryAllLinearRegressions(thedataframe, topredict, frompredict, dictofunknown={}, num_iterations=100, \n",
    "                            rounddecimals=\"none\", printout=False, classifier=False, transform=\"none\"):\n",
    "    # We begin by making the input data from thedataframe\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    \n",
    "    modelstotryout = [linear_model.LinearRegression, linear_model.RidgeCV, linear_model.LassoCV, \n",
    "                      linear_model.BayesianRidge, linear_model.SGDRegressor]\n",
    "    \n",
    "    if classifier==True:\n",
    "        randomprediction = randomPrediction(y, rounddecimals=rounddecimals, num_iterations=num_iterations)\n",
    "    modelandresults = {}\n",
    "    for model in modelstotryout:\n",
    "        modelname = getModelName(model)\n",
    "        if printout==True:\n",
    "            print modelname\n",
    "        transformtouse = transform\n",
    "        if model==linear_model.SGDRegressor:\n",
    "            transformtouse = \"scale\"\n",
    "        if \"n_alphas\" in model().get_params():\n",
    "            modeldata = getParametersAndPredictionScore(X, y, model, rounddecimals=rounddecimals, \n",
    "                                                        num_iterations=num_iterations, n_alphas=3,\n",
    "                                                        classifier=classifier, transform=transformtouse)\n",
    "        else:\n",
    "            modeldata = getParametersAndPredictionScore(X, y, model, rounddecimals=rounddecimals, \n",
    "                                                        num_iterations=num_iterations, classifier=classifier,\n",
    "                                                        transform=transformtouse)\n",
    "        modelandresults[modelname] = modeldata\n",
    "        if printout==True:\n",
    "            print u\"\\tThe score on the fit is %f \\u00B1 %f (1.0 is perfect)\" %modeldata[2]\n",
    "            if classifier==True:\n",
    "                print u\"\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modeldata[-1]\n",
    "                print u\"\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "    return modelandresults\n",
    "\n",
    "# Makes a scatterplot of the error in prediction, versus the predicted value, to see if there\n",
    "# is any heteroscedasticity, i.e. to see if the error bars of our prediction depend on the value\n",
    "# of the prediction. Returns the R^2 value of the prediction.\n",
    "def plotErrors(thedataframe, topredict, frompredict, chosenmodel, dictofregressions, \n",
    "               rounddecimals=\"none\", minvalue=\"none\", maxvalue=\"none\", ax=\"none\", dictofunknown={}, \n",
    "               alpha=0.3, transform=\"none\"):\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    if transform!=\"none\":\n",
    "        X = transformData(X, transform=transform)\n",
    "    coefficients = dictofregressions[chosenmodel][1]\n",
    "    intercept = dictofregressions[chosenmodel][0]\n",
    "    prediction = np.dot(X,coefficients) + intercept\n",
    "    if transform!=\"none\":\n",
    "        prediction = inverseTransformData(prediction, transform=transform)\n",
    "    if rounddecimals!=\"none\":\n",
    "        prediction = np.round(prediction, decimals=rounddecimals)\n",
    "    if minvalue!=\"none\":\n",
    "        prediction[prediction < minvalue] = minvalue\n",
    "    if maxvalue!=\"none\":\n",
    "        prediction[prediction > maxvalue] = maxvalue\n",
    "    if ax==\"none\":\n",
    "        plt.scatter(prediction, prediction - y, alpha=alpha)\n",
    "        plt.xlabel(\"Predicted \" + topredict)\n",
    "        plt.ylabel(\"Error in prediction\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        ax.scatter(prediction, prediction - y, alpha=alpha)\n",
    "        ax.set_xlabel(\"Predicted \" + topredict)\n",
    "        ax.set_ylabel(\"Error in prediction\")\n",
    "    print \"Mean: %f\" %np.mean(prediction - y)\n",
    "    print \"Std: %f\" %np.std(prediction - y)\n",
    "    print \"R^2: %f\" %(np.var(prediction) / np.var(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown):\n",
    "    tousedataframe = removeAutogeneratedData(thedataframe, frompredict + [topredict], dictofunknown)\n",
    "    X = tousedataframe[frompredict].as_matrix()\n",
    "    y = tousedataframe[topredict].as_matrix()\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for using regressions to fill in unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Makes a dataframe where the values of topredict have been predicted using frompredict, with the regression \n",
    "# model called chosenmodel (a string). For this we need to have found all regressions, placed in the variable\n",
    "# dictofregressions.\n",
    "def incorporateRegressionPrediction(inputdataframe, topredict, frompredict, chosenmodel, dictofregressions,\n",
    "                                    rounddecimals=\"none\", minvalue=\"none\", maxvalue=\"none\", transform=\"none\"):\n",
    "    thedataframe = deepcopy(inputdataframe)\n",
    "    if topredict in dictofunknown:\n",
    "        topredictframe = thedataframe[thedataframe[topredict]==dictofunknown[topredict]]\n",
    "    else:\n",
    "        topredictframe = thedataframe[thedataframe[topredict].isnull()]\n",
    "    ind = topredictframe.index\n",
    "    dataX = topredictframe[frompredict].as_matrix()\n",
    "    if transform!=\"none\":\n",
    "        dataX = transformData(dataX, transform=transform)\n",
    "    if len(dataX) > 0:\n",
    "        coefficients = dictofregressions[chosenmodel][1]\n",
    "        intercept = dictofregressions[chosenmodel][0]\n",
    "        prediction = np.dot(dataX,coefficients) + intercept\n",
    "        if transform!=\"none\":\n",
    "            prediction = inverseTransformData(prediction, transform=transform)\n",
    "        if rounddecimals!=\"none\":\n",
    "            prediction = np.round(prediction, decimals=rounddecimals)\n",
    "        if minvalue!=\"none\":\n",
    "            prediction[prediction < minvalue] = minvalue\n",
    "        if maxvalue!=\"none\":\n",
    "            prediction[prediction > maxvalue] = maxvalue\n",
    "        thedataframe.loc[ind, topredict] = prediction\n",
    "    return thedataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use linear regression for missing data\n",
    "\n",
    "After having tried various options, it seems like using transform=\"log\" we're able to avoid most of the heteroscedasticity we otherwise get with a straightforward linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "\tThe score on the fit is 0.228596 ± 0.075181 (1.0 is perfect)\n",
      "RidgeCV\n",
      "\tThe score on the fit is 0.229066 ± 0.075665 (1.0 is perfect)\n",
      "LassoCV\n",
      "\tThe score on the fit is 0.230709 ± 0.075397 (1.0 is perfect)\n",
      "BayesianRidge\n",
      "\tThe score on the fit is 0.228654 ± 0.075818 (1.0 is perfect)\n",
      "SGDRegressor\n",
      "\tThe score on the fit is 0.210884 ± 0.062576 (1.0 is perfect)\n"
     ]
    }
   ],
   "source": [
    "numericdataframe = makeAllColumnsNumeric(thedataframe, dictofordinals, datacateg)[0]\n",
    "topredict = \"Age\"\n",
    "frompredict = ['Pclass', 'Sex', 'SibSp', 'Parch', \"Fare\", \"Cabin\", 'Embarked']\n",
    "tousedataframe = deepcopy(numericdataframe)\n",
    "transform = \"log\"\n",
    "\n",
    "linearregressions = tryAllLinearRegressions(tousedataframe, topredict, frompredict,\n",
    "                                            dictofunknown={'Age': 29.6991176471}, printout=True, num_iterations=100, \n",
    "                                            rounddecimals=\"none\", classifier=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check heteroscedasticity of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEPCAYAAACzwehFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRhJ+f8I0kiNYvuZJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc0e0c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -3.664750\n",
      "Std: 12.411338\n",
      "R^2: 0.493061\n"
     ]
    }
   ],
   "source": [
    "chosenmodel = \"LinearRegression\"\n",
    "dictofregressions = linearregressions\n",
    "rounddecimals=0\n",
    "minvalue=0\n",
    "maxvalue=100\n",
    "\n",
    "plotErrors(tousedataframe, topredict, frompredict, chosenmodel, dictofregressions,\n",
    "           rounddecimals=rounddecimals, minvalue=minvalue, maxvalue=maxvalue, ax=\"none\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new datafram with predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newdataframe = incorporateRegressionPrediction(tousedataframe, topredict, frompredict, chosenmodel, dictofregressions,\n",
    "                                               rounddecimals=rounddecimals, minvalue=minvalue, maxvalue=maxvalue, \n",
    "                                               transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Generic functions for making predictions and checking their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrediction(trainX, trainy, dataX, predictionmethod, transform=\"none\",**kwargs):\n",
    "    (Xtraining, ytraining) = (trainX, trainy)\n",
    "    X = dataX\n",
    "    if transform!=\"none\":\n",
    "        Xtraining = transformData(trainX, transform=transform)\n",
    "        X = transformData(dataX, transform=transform)\n",
    "    if transform==\"scale\":\n",
    "        (Xtraining, X) = scaleData(Xtraining, X)\n",
    "    predictor = predictionmethod(**kwargs)\n",
    "    predictor.fit(Xtraining,ytraining)\n",
    "    predictedy = predictor.predict(X)\n",
    "    return predictedy\n",
    "\n",
    "def getPredictionProba(trainX, trainy, dataX, predictionmethod, transform=\"none\",**kwargs):\n",
    "    (Xtraining, ytraining) = (trainX, trainy)\n",
    "    X = dataX\n",
    "    if transform!=\"none\":\n",
    "        Xtraining = transformData(trainX, transform=transform)\n",
    "        X = transformData(dataX, transform=transform)\n",
    "    if transform==\"scale\":\n",
    "        (Xtraining, X) = scaleData(Xtraining, X)\n",
    "    predictor = predictionmethod(**kwargs)\n",
    "    predictor.fit(Xtraining,ytraining)\n",
    "    predictedy = predictor.predict_proba(X)\n",
    "    return predictedy\n",
    "\n",
    "def getPredictionScore(inputX, inputy, predictionmethod, num_iterations=1, transform=\"none\",**kwargs):\n",
    "    (X, y) = (inputX, inputy)\n",
    "    if transform!=\"none\":\n",
    "        X = transformData(inputX, transform=transform)\n",
    "    score = []\n",
    "    for iter in range(num_iterations):\n",
    "        kfoldindices = KFold(len(y),n_folds=5,shuffle=True)\n",
    "        for trainindex, testindex in kfoldindices:\n",
    "            Xtrain, Xtest = X[trainindex], X[testindex]\n",
    "            if transform==\"scale\":\n",
    "                (Xtrain, Xtest) = scaleData(Xtrain, Xtest)\n",
    "            ytrain, ytest = y[trainindex], y[testindex]\n",
    "            predictor = predictionmethod(**kwargs)\n",
    "            predictor.fit(Xtrain,ytrain)\n",
    "            score.append(predictor.score(Xtest,ytest))\n",
    "    score = (np.mean(score),np.std(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for making classification through regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryAllLinRegClassifiers(thedataframe, topredict, frompredict, dictofunknown={}, num_iterations=30, \n",
    "                            rounddecimals=0, printout=True, transform=\"scale\", polynomialdegrees=[1]):\n",
    "    linregs = tryPolynomialRegressions(thedataframe, topredict, frompredict, polynomialdegrees,\n",
    "                                       dictofunknown=dictofunknown, num_iterations=num_iterations,\n",
    "                                       rounddecimals=rounddecimals, printout=printout, classifier=True, transform=transform)\n",
    "    otherregs = tryOtherRegressions(thedataframe, topredict, frompredict,\n",
    "                                    dictofunknown=dictofunknown, num_iterations=num_iterations,\n",
    "                                    rounddecimals=rounddecimals, printout=printout, classifier=True, transform=\"none\")\n",
    "    linregs.update(otherregs)\n",
    "    regclassifiers = tryAllRegressionClassifiers(thedataframe, topredict, frompredict, polynomialdegrees, \n",
    "                                                 dictofunknown=dictofunknown, num_iterations=num_iterations, \n",
    "                                                 printout=printout, transform=transform)\n",
    "    linregs.update(regclassifiers)\n",
    "    otherclass = tryOtherClassifiers(thedataframe, topredict, frompredict, dictofunknown=dictofunknown, \n",
    "                                     num_iterations=num_iterations, printout=printout, transform=transform)\n",
    "    linregs.update(otherclass)\n",
    "    return linregs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryAllRegressionClassifiers(thedataframe, topredict, frompredict, polynomialdegrees, dictofunknown={}, num_iterations=100, \n",
    "                            printout=False, transform=\"none\"):\n",
    "    # We begin by making the input data from thedataframe\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    \n",
    "    modelstotryout = [linear_model.LogisticRegression, linear_model.LogisticRegressionCV, linear_model.SGDClassifier,\n",
    "                      linear_model.Perceptron]\n",
    "    \n",
    "    if type(polynomialdegrees)==int:\n",
    "        alldegrees = [polynomialdegrees]\n",
    "    else:\n",
    "        alldegrees = polynomialdegrees\n",
    "    \n",
    "    randomprediction = randomPrediction(y, num_iterations=num_iterations)\n",
    "    \n",
    "    modelandresults = {}\n",
    "    for model in modelstotryout:\n",
    "        modelname = getModelName(model)\n",
    "        if printout==True:\n",
    "            print modelname\n",
    "        transformtouse = transform\n",
    "        if model==linear_model.SGDClassifier or model==linear_model.Perceptron:\n",
    "            transformtouse = \"scale\"\n",
    "        for polydegree in alldegrees:\n",
    "            poly = PolynomialFeatures(degree=polydegree)\n",
    "            polyX = poly.fit_transform(X)\n",
    "            if printout==True:\n",
    "                print \"\\tPolynomial of degree \" + str(polydegree)\n",
    "            if \"n_alphas\" in model().get_params(): \n",
    "                modelscore = getPredictionScore(polyX, y, model, num_iterations=num_iterations, n_alphas=3, \n",
    "                                                transform=transformtouse)\n",
    "            else:\n",
    "                modelscore = getPredictionScore(polyX, y, model, num_iterations=num_iterations, transform=transformtouse)\n",
    "            modelandresults[modelname] = modelscore\n",
    "            if printout==True:\n",
    "                print u\"\\t\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modelscore\n",
    "                print u\"\\t\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "            # Now we'll do the Bagging version of this same model\n",
    "            if printout==True:\n",
    "                print \"\\t    Bagging version\"\n",
    "            if \"n_alphas\" in model().get_params(): \n",
    "                modelscore = getPredictionScore(X, y, BaggingClassifier, num_iterations=2, transform=transformtouse,\n",
    "                                            base_estimator=model(n_alphas=3), n_estimators=30)\n",
    "            else:\n",
    "                modelscore = getPredictionScore(X, y, BaggingClassifier, num_iterations=2, transform=transformtouse,\n",
    "                                            base_estimator=model(), n_estimators=30)\n",
    "            modelandresults[(\"Bagging \" + modelname)] = modelscore\n",
    "            if printout==True:\n",
    "                print u\"\\t\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modelscore\n",
    "                print u\"\\t\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "    return modelandresults\n",
    "\n",
    "def tryPolynomialRegressions(thedataframe, topredict, frompredict, polynomialdegrees, dictofunknown={}, num_iterations=100, \n",
    "                             rounddecimals=\"none\", printout=False, classifier=False, transform=\"none\"):\n",
    "    # We begin by making the input data from thedataframe\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    \n",
    "    modelstotryout = [linear_model.LinearRegression, linear_model.RidgeCV, linear_model.LassoCV, \n",
    "                      linear_model.BayesianRidge, linear_model.SGDRegressor]\n",
    "    \n",
    "    if type(polynomialdegrees)==int:\n",
    "        alldegrees = [polynomialdegrees]\n",
    "    else:\n",
    "        alldegrees = polynomialdegrees\n",
    "    \n",
    "    if classifier==True:\n",
    "        randomprediction = randomPrediction(y, rounddecimals=rounddecimals, num_iterations=num_iterations)\n",
    "    modelandresults = {}\n",
    "    for model in modelstotryout:\n",
    "        modelname = getModelName(model)\n",
    "        if printout==True:\n",
    "            print modelname\n",
    "        transformtouse = transform\n",
    "        if model==linear_model.SGDRegressor or model==linear_model.Perceptron:\n",
    "            transformtouse = \"scale\"\n",
    "        for polydegree in alldegrees:\n",
    "            poly = PolynomialFeatures(degree=polydegree)\n",
    "            polyX = poly.fit_transform(X)\n",
    "            if printout==True:\n",
    "                print \"\\tPolynomial of degree \" + str(polydegree)\n",
    "            if \"n_alphas\" in model().get_params():\n",
    "                modeldata = getPredictionPercentage(polyX, y, model, rounddecimals=rounddecimals, \n",
    "                                                            num_iterations=num_iterations, n_alphas=3,\n",
    "                                                            classifier=classifier, transform=transformtouse)\n",
    "            else:\n",
    "                modeldata = getPredictionPercentage(polyX, y, model, rounddecimals=rounddecimals, \n",
    "                                                            num_iterations=num_iterations, classifier=classifier,\n",
    "                                                            transform=transformtouse)\n",
    "            modelandresults[(modelname,polydegree)] = modeldata\n",
    "            if printout==True:\n",
    "                print u\"\\t\\tThe score on the fit is %f \\u00B1 %f (1.0 is perfect)\" %modeldata[-2]\n",
    "                if classifier==True:\n",
    "                    print u\"\\t\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modeldata[-1]\n",
    "                    print u\"\\t\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "            # Now we'll do the Bagging version of this same model\n",
    "            if printout==True:\n",
    "                print \"\\t    Bagging version\"\n",
    "            if \"n_alphas\" in model().get_params(): \n",
    "                modeldata = getPredictionPercentage(polyX, y, BaggingRegressor, rounddecimals=rounddecimals, \n",
    "                                                             num_iterations=2, classifier=classifier,\n",
    "                                                             transform=transformtouse, base_estimator=model(n_alphas=3),\n",
    "                                                             n_estimators=30)\n",
    "            else:\n",
    "                modeldata = getPredictionPercentage(polyX, y, BaggingRegressor, rounddecimals=rounddecimals, \n",
    "                                                             num_iterations=2, classifier=classifier,\n",
    "                                                             transform=transformtouse, base_estimator=model(),\n",
    "                                                             n_estimators=30)\n",
    "            modelandresults[(\"Bagging \" + modelname)] = modeldata\n",
    "            if printout==True:\n",
    "                print u\"\\t\\tThe score on the fit is %f \\u00B1 %f (1.0 is perfect)\" %modeldata[-2]\n",
    "                if classifier==True:\n",
    "                    print u\"\\t\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modeldata[-1]\n",
    "                    print u\"\\t\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "    return modelandresults\n",
    "\n",
    "def tryOtherRegressions(thedataframe, topredict, frompredict, dictofunknown={}, num_iterations=100, \n",
    "                        rounddecimals=\"none\", printout=False, classifier=False, transform=\"none\"):\n",
    "    # We begin by making the input data from thedataframe\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    \n",
    "    modelstotryout = [KernelRidge, GaussianProcessRegressor]\n",
    "       \n",
    "    if classifier==True:\n",
    "        randomprediction = randomPrediction(y, rounddecimals=rounddecimals, num_iterations=num_iterations)\n",
    "    modelandresults = {}\n",
    "    for model in modelstotryout:\n",
    "        modelname = getModelName(model)\n",
    "        if printout==True:\n",
    "            print modelname\n",
    "        transformtouse = transform\n",
    "        if model==linear_model.SGDRegressor or model==linear_model.Perceptron:\n",
    "            transformtouse = \"scale\"\n",
    "        if \"n_alphas\" in model().get_params():\n",
    "            modeldata = getPredictionPercentage(X, y, model, rounddecimals=rounddecimals, \n",
    "                                                num_iterations=num_iterations, n_alphas=3,\n",
    "                                                classifier=classifier, transform=transformtouse)\n",
    "        else:\n",
    "            modeldata = getPredictionPercentage(X, y, model, rounddecimals=rounddecimals, \n",
    "                                                num_iterations=num_iterations, classifier=classifier,\n",
    "                                                transform=transformtouse)\n",
    "        modelandresults[modelname] = modeldata\n",
    "        if printout==True:\n",
    "            print u\"\\tThe score on the fit is %f \\u00B1 %f (1.0 is perfect)\" %modeldata[-2]\n",
    "            if classifier==True:\n",
    "                print u\"\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modeldata[-1]\n",
    "                print u\"\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "        # Now we'll do the Bagging version of this same model\n",
    "        if printout==True:\n",
    "            print \"    Bagging version\"\n",
    "        if \"n_alphas\" in model().get_params(): \n",
    "            modeldata = getPredictionPercentage(X, y, BaggingRegressor, rounddecimals=rounddecimals, \n",
    "                                                num_iterations=2, classifier=classifier,\n",
    "                                                transform=transformtouse, base_estimator=model(n_alphas=3),\n",
    "                                                n_estimators=30)\n",
    "        else:\n",
    "            modeldata = getPredictionPercentage(X, y, BaggingRegressor, rounddecimals=rounddecimals, \n",
    "                                                num_iterations=2, classifier=classifier,\n",
    "                                                transform=transformtouse, base_estimator=model(),\n",
    "                                                n_estimators=30)\n",
    "        modelandresults[(\"Bagging \" + modelname)] = modeldata\n",
    "        if printout==True:\n",
    "            print u\"\\tThe score on the fit is %f \\u00B1 %f (1.0 is perfect)\" %modeldata[-2]\n",
    "            if classifier==True:\n",
    "                print u\"\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modeldata[-1]\n",
    "                print u\"\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "    return modelandresults\n",
    "\n",
    "def tryOtherClassifiers(thedataframe, topredict, frompredict, dictofunknown={}, num_iterations=100, \n",
    "                            printout=False, transform=\"none\"):\n",
    "    # We begin by making the input data from thedataframe\n",
    "    (X,y) = makeDataFromModels(thedataframe, topredict, frompredict, dictofunknown)\n",
    "    \n",
    "    modelstotryout = [SVC, KNeighborsClassifier, NearestCentroid, GaussianProcessClassifier, GaussianNB, \n",
    "                      DecisionTreeClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,\n",
    "                      GradientBoostingClassifier]\n",
    "      \n",
    "    randomprediction = randomPrediction(y, num_iterations=num_iterations)\n",
    "    \n",
    "    modelandresults = {}\n",
    "    for model in modelstotryout:\n",
    "        modelname = getModelName(model)\n",
    "        if printout==True:\n",
    "            print modelname\n",
    "        transformtouse = transform\n",
    "        if model==linear_model.SGDClassifier or model==linear_model.Perceptron or model==SVC:\n",
    "            transformtouse = \"scale\"\n",
    "        if \"n_alphas\" in model().get_params():\n",
    "            modelscore = getPredictionScore(X, y, model, num_iterations=num_iterations, n_alphas=3, \n",
    "                                            transform=transformtouse)\n",
    "        else:\n",
    "            modelscore = getPredictionScore(X, y, model, num_iterations=num_iterations, transform=transformtouse)\n",
    "        modelandresults[modelname] = modelscore\n",
    "        if printout==True:\n",
    "            print u\"\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modelscore\n",
    "            print u\"\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "        # Now we'll do the Bagging version of this same model\n",
    "        if model!=RandomForestClassifier and model!=ExtraTreesClassifier:\n",
    "            if printout==True:\n",
    "                print \"    Bagging version\"\n",
    "            if \"n_alphas\" in model().get_params(): \n",
    "                modelscore = getPredictionScore(X, y, BaggingClassifier, num_iterations=2, transform=transformtouse,\n",
    "                                            base_estimator=model(n_alphas=3), n_estimators=30)\n",
    "            else:\n",
    "                modelscore = getPredictionScore(X, y, BaggingClassifier, num_iterations=2, transform=transformtouse,\n",
    "                                            base_estimator=model(), n_estimators=30)\n",
    "            modelandresults[(\"Bagging \" + modelname)] = modelscore\n",
    "            if printout==True:\n",
    "                print u\"\\tThe percentage of correct identifications is: %f \\u00B1 %f\" %modelscore\n",
    "                print u\"\\t         Benchmark from random prediction is: %f \\u00B1 %f\" %randomprediction\n",
    "    return modelandresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe score on the fit is 0.383243 ± 0.065048 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.791469 ± 0.030144\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.387069 ± 0.069074 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.792799 ± 0.025757\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe score on the fit is 0.400357 ± 0.064851 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.808009 ± 0.032541\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.391988 ± 0.076078 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.811630 ± 0.038480\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe score on the fit is -2324871405332914503680.000000 ± 16269709327984224108544.000000 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.765306 ± 0.028434\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is -17293302607804196864.000000 ± 34571313953055354880.000000 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.743744 ± 0.034618\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "RidgeCV\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe score on the fit is 0.387475 ± 0.051760 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.789612 ± 0.025970\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.388561 ± 0.055300 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.795643 ± 0.023258\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe score on the fit is 0.409852 ± 0.065705 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.816544 ± 0.028127\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.418706 ± 0.061428 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.815656 ± 0.026925\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe score on the fit is 0.404953 ± 0.077357 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.808653 ± 0.026134\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.396204 ± 0.095690 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.805341 ± 0.034826\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "LassoCV\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe score on the fit is 0.385569 ± 0.067785 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.794291 ± 0.028700\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.385933 ± 0.052689 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.792237 ± 0.024519\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniele\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe score on the fit is 0.407768 ± 0.056624 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.812329 ± 0.027881\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.415655 ± 0.036060 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.817899 ± 0.020535\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe score on the fit is 0.405937 ± 0.074618 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.812338 ± 0.029311\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.399471 ± 0.096720 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.813364 ± 0.024162\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "BayesianRidge\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe score on the fit is 0.385770 ± 0.044067 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.793729 ± 0.022178\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.381469 ± 0.051410 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.793935 ± 0.028600\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe score on the fit is 0.399447 ± 0.087370 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.809913 ± 0.030706\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.407833 ± 0.052062 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.816747 ± 0.024515\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe score on the fit is 0.407932 ± 0.068364 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.814498 ± 0.028410\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.415445 ± 0.038815 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.813925 ± 0.014288\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "SGDRegressor\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe score on the fit is 0.384234 ± 0.055288 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.792805 ± 0.028072\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.389846 ± 0.053579 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.795630 ± 0.026497\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe score on the fit is 0.398462 ± 0.060611 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.807312 ± 0.020509\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is 0.307015 ± 0.203987 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.797370 ± 0.033752\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe score on the fit is -43014067.356809 ± 214163119.759019 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.070500 ± 0.151048\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "\t    Bagging version\n",
      "\t\tThe score on the fit is -157188668683108057088.000000 ± 260469242817899790336.000000 (1.0 is perfect)\n",
      "\t\tThe percentage of correct identifications is: 0.000000 ± 0.000000\n",
      "\t\t         Benchmark from random prediction is: 0.610743 ± 0.034591\n",
      "KernelRidge\n",
      "\tThe score on the fit is 0.146486 ± 0.059079 (1.0 is perfect)\n",
      "\tThe percentage of correct identifications is: 0.744042 ± 0.023215\n",
      "\t         Benchmark from random prediction is: 0.610718 ± 0.034285\n",
      "    Bagging version\n",
      "\tThe score on the fit is 0.140759 ± 0.081078 (1.0 is perfect)\n",
      "\tThe percentage of correct identifications is: 0.743172 ± 0.027274\n",
      "\t         Benchmark from random prediction is: 0.610718 ± 0.034285\n",
      "GaussianProcessRegressor\n",
      "\tThe score on the fit is -52123.447501 ± 133300.343970 (1.0 is perfect)\n",
      "\tThe percentage of correct identifications is: 0.495200 ± 0.032457\n",
      "\t         Benchmark from random prediction is: 0.610718 ± 0.034285\n",
      "    Bagging version\n",
      "\tThe score on the fit is -4011.273507 ± 4853.836880 (1.0 is perfect)\n",
      "\tThe percentage of correct identifications is: 0.448640 ± 0.046772\n",
      "\t         Benchmark from random prediction is: 0.610718 ± 0.034285\n",
      "LogisticRegression\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe percentage of correct identifications is: 0.790304 ± 0.027324\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.788812 ± 0.034001\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe percentage of correct identifications is: 0.813010 ± 0.027862\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.792841 ± 0.031156\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe percentage of correct identifications is: 0.809812 ± 0.029146\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.787120 ± 0.021165\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "LogisticRegressionCV\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe percentage of correct identifications is: 0.792574 ± 0.027793\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.796815 ± 0.030214\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe percentage of correct identifications is: 0.810492 ± 0.026608\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.789961 ± 0.026461\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe percentage of correct identifications is: 0.811649 ± 0.022990\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.791711 ± 0.033006\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "SGDClassifier\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe percentage of correct identifications is: 0.732056 ± 0.057315\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.783143 ± 0.037388\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe percentage of correct identifications is: 0.733457 ± 0.075551\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.784818 ± 0.027750\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe percentage of correct identifications is: 0.747810 ± 0.077430\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.792224 ± 0.036045\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "Perceptron\n",
      "\tPolynomial of degree 1\n",
      "\t\tThe percentage of correct identifications is: 0.713930 ± 0.071745\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.782552 ± 0.025174\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 2\n",
      "\t\tThe percentage of correct identifications is: 0.752058 ± 0.064724\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.781373 ± 0.019325\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\tPolynomial of degree 3\n",
      "\t\tThe percentage of correct identifications is: 0.761293 ± 0.063390\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "\t    Bagging version\n",
      "\t\tThe percentage of correct identifications is: 0.784812 ± 0.019509\n",
      "\t\t         Benchmark from random prediction is: 0.610756 ± 0.035652\n",
      "SVC\n",
      "\tThe percentage of correct identifications is: 0.817136 ± 0.025780\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.816802 ± 0.020425\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "KNeighborsClassifier\n",
      "\tThe percentage of correct identifications is: 0.801722 ± 0.029063\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.803026 ± 0.031350\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "NearestCentroid\n",
      "\tThe percentage of correct identifications is: 0.766882 ± 0.024388\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.768825 ± 0.019757\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "GaussianProcessClassifier\n",
      "\tThe percentage of correct identifications is: 0.811663 ± 0.030188\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.811081 ± 0.022118\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "GaussianNB\n",
      "\tThe percentage of correct identifications is: 0.775565 ± 0.026337\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.774545 ± 0.034508\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "DecisionTreeClassifier\n",
      "\tThe percentage of correct identifications is: 0.776590 ± 0.035716\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.808221 ± 0.020454\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "RandomForestClassifier\n",
      "\tThe percentage of correct identifications is: 0.800908 ± 0.026686\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "ExtraTreesClassifier\n",
      "\tThe percentage of correct identifications is: 0.787206 ± 0.030310\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "AdaBoostClassifier\n",
      "\tThe percentage of correct identifications is: 0.799094 ± 0.029781\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.797964 ± 0.022979\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "GradientBoostingClassifier\n",
      "\tThe percentage of correct identifications is: 0.815987 ± 0.020717\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n",
      "    Bagging version\n",
      "\tThe percentage of correct identifications is: 0.823643 ± 0.020684\n",
      "\t         Benchmark from random prediction is: 0.610725 ± 0.033359\n"
     ]
    }
   ],
   "source": [
    "topredict = \"Survived\"\n",
    "frompredict = ['Pclass', 'Sex', \"Age\", 'SibSp', 'Parch', \"Fare\", \"Cabin\", 'Embarked']\n",
    "\n",
    "allmodelsandresults = tryAllLinRegClassifiers(numericdataframe, topredict, frompredict, num_iterations=10, \n",
    "                        transform=\"scale\", polynomialdegrees=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81643636363636352, 0.024318744518657572)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericdataframe = makeAllColumnsNumeric(thedataframe, dictofordinals, datacateg)[0]\n",
    "frompredict = ['Pclass', 'Sex', \"Age\", 'SibSp', 'Parch', \"Fare\", \"Cabin\", 'Embarked']\n",
    "#frompredict = ['Pclass', 'Sex', 'SibSp']\n",
    "\n",
    "(X,y) = makeDataFromModels(numericdataframe, topredict, frompredict, {})\n",
    "getPredictionScore(X, y, SVC, num_iterations=20, transform=\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find ideal number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numericdataframe = makeAllColumnsNumeric(thedataframe, dictofordinals, datacateg)[0]\n",
    "frompredict = ['Pclass', 'Sex', \"Age\", 'SibSp', 'Parch', \"Fare\", \"Cabin\", 'Embarked']\n",
    "#frompredict = ['Pclass', 'Sex', 'SibSp']\n",
    "\n",
    "(X,y) = makeDataFromModels(numericdataframe, topredict, frompredict, {})\n",
    "allscores = []\n",
    "for jj in range(1,20):\n",
    "    allscores.append(getPredictionScore(X, y, KNeighborsClassifier, num_iterations=30, transform=\"scale\", n_neighbors=jj)[0])\n",
    "\n",
    "# if we decrease wrt a value twice in a row, we've reached the max\n",
    "bestnumneighbors = list(np.append(\n",
    "    (np.array(allscores[2:]) - np.array(allscores[:-2]) < 0).astype(int), \n",
    "    1) * (np.array(allscores[1:]) - np.array(allscores[:-1]) < 0).astype(int)).index(1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal number is 13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nphyxpG+MMeXI/wcRIDRFFJRWHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc12c860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "polyX = poly.fit_transform((np.arange(len(allscores))+1).reshape(-1, 1))\n",
    "linreg = linear_model.LinearRegression()\n",
    "linreg.fit(polyX,np.array(allscores).reshape(-1, 1))\n",
    "xs = np.linspace(0,18)\n",
    "plt.plot(xs, linreg.intercept_ + linreg.coef_[0][0] + linreg.coef_[0][1]*xs + linreg.coef_[0][2]*(xs**2))\n",
    "plt.plot(allscores)\n",
    "print \"Ideal number is %d\" %np.round(-linreg.coef_[0][1]/(2*linreg.coef_[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82166623376623382, 0.027986421212895671)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPredictAccur(predictionmethod):\n",
    "    try:\n",
    "        predictionaccuracy = allmodelsandresults[(getModelName(predictionmethod),polydegree)][-1][0]\n",
    "    except KeyError:\n",
    "        predictionaccuracy = allmodelsandresults[getModelName(predictionmethod)][0]\n",
    "    return predictionaccuracy\n",
    "\n",
    "# ================================================\n",
    "# THESE FUNCTIONS ARE FOR ESTIMATING THE CORRECT VALUES BASED ON A BAYES-LIKE PROB OF THE RIGHT ANSWER\n",
    "def getprob(result, event, prob):\n",
    "    if result==event:\n",
    "        chance = prob \n",
    "    else:\n",
    "        chance = 1 - prob\n",
    "    return chance\n",
    "\n",
    "def getConsensusResult(resultlist, probs):\n",
    "    prob1 = [getprob(result, 1,prob) for prob,result in zip(probs,resultlist)]\n",
    "    prob0 = [getprob(result, 0,prob) for prob,result in zip(probs,resultlist)]\n",
    "    #ratio = (np.mean(y)/(1-np.mean(y)))* np.prod(prob1)/np.prod(prob0)\n",
    "    ratio = np.prod(prob1)/np.prod(prob0)\n",
    "    probshouldbe1 = ratio / (1+ratio)\n",
    "    consensusresult = np.round(probshouldbe1)\n",
    "    return consensusresult\n",
    "# ================================================\n",
    "\n",
    "num_iterations = 10\n",
    "polydegree = 2\n",
    "modelstotryout = [linear_model.RidgeCV, linear_model.SGDRegressor, linear_model.LogisticRegression, \n",
    "                  SVC, KNeighborsClassifier, NearestCentroid, RandomForestClassifier, ExtraTreesClassifier,\n",
    "                  AdaBoostClassifier, GradientBoostingClassifier]\n",
    "\n",
    "poly = PolynomialFeatures(degree=polydegree)\n",
    "probs = [getPredictAccur(predictionmethod) for predictionmethod in modelstotryout]\n",
    "totalvotes = np.sum(probs)\n",
    "score = []\n",
    "for iter in range(num_iterations):\n",
    "    kfoldindices = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    for trainindex, testindex in kfoldindices:\n",
    "        Xtrain, Xtest = X[trainindex], X[testindex]\n",
    "        ytrain, ytest = y[trainindex], y[testindex]\n",
    "        ensemblepredictions = []\n",
    "        for predictionmethod in modelstotryout:\n",
    "            if predictionmethod in [linear_model.RidgeCV, linear_model.SGDRegressor, linear_model.LogisticRegressionCV]:\n",
    "                Xtraintouse = poly.fit_transform(Xtrain)\n",
    "                Xtesttouse = poly.fit_transform(Xtest)\n",
    "            else:\n",
    "                Xtraintouse, Xtesttouse = Xtrain, Xtest\n",
    "            (Xtraintouse, Xtesttouse) = scaleData(Xtraintouse, Xtesttouse)\n",
    "            #if predictionmethod==DecisionTreeClassifier:\n",
    "            #    predictor = predictionmethod(max_depth=6, min_samples_leaf=5)\n",
    "            #elif predictionmethod==KNeighborsClassifier:\n",
    "            #    predictor = predictionmethod(n_neighbors=14)\n",
    "            #else:\n",
    "            #    predictor = predictionmethod()\n",
    "            predictor = predictionmethod()\n",
    "            predictor.fit(Xtraintouse,ytrain)\n",
    "            predictionaccuracy = getPredictAccur(predictionmethod)\n",
    "            prediction = np.round(predictor.predict(Xtesttouse), decimals=0)*predictionaccuracy\n",
    "            ensemblepredictions.append(prediction)\n",
    "        masterprediction = np.round(np.dot(np.transpose(np.array(ensemblepredictions)), \n",
    "                                           np.full(len(modelstotryout), 1.0)) / totalvotes)\n",
    "        # ================================================\n",
    "        # ALTERNATIVE METHOD, USING A BAYES-LIKE PROBABILITY ESTIMATE FOR WHICH ANSWER IS RIGHT\n",
    "        #rounded = np.round(np.transpose(np.array(ensemblepredictions)))\n",
    "        #masterprediction = np.array([getConsensusResult(resultlist,probs) for resultlist in rounded])\n",
    "        # ================================================\n",
    "        score.append(np.mean(masterprediction==ytest))\n",
    "        \n",
    "(np.mean(score),np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82421168831168823, 0.017769286875963353)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HERE IS SOMETHING THAT DOES NEARLY EXACTLY THE SAME THING (EXCEPT A LESS PRECISE COMPUTATION OF SCORES)\n",
    "def daname(clf):\n",
    "    return str(clf)[:str(clf).index(\"(\")]\n",
    "\n",
    "Xscaled = scaleData(X,X)[0]\n",
    "\n",
    "clf2 = linear_model.LogisticRegression()\n",
    "clf3 = SVC()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = NearestCentroid()\n",
    "clf6 = RandomForestClassifier(n_estimators=30)\n",
    "clf7 = ExtraTreesClassifier()\n",
    "clf8 = AdaBoostClassifier()\n",
    "clf9 = GradientBoostingClassifier()\n",
    "theweights = [allmodelsandresults[daname(clf)][0] for clf in [clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9]]\n",
    "allscores = np.array([])\n",
    "for ii in range(10):\n",
    "    ensemble = VotingClassifier(estimators=[('2', clf2), ('3', clf3), ('4', clf4), ('5', clf5), \n",
    "                                        ('6', clf6), ('7', clf7), ('8', clf8), ('9', clf9)], voting='hard', weights=theweights)\n",
    "    scores = cross_val_score(ensemble, Xscaled, y, cv=5)\n",
    "    allscores = np.append(allscores,scores)\n",
    "(np.mean(allscores), np.std(allscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82535714285714301"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IT WORKS ALSO WITH GRIDCV!\n",
    "ensemble = VotingClassifier(estimators=[('2', clf2), ('3', clf3),  ('4', clf4),  ('5', clf5), \n",
    "                                        ('6', clf6), ('7', clf7), ('8', clf8), ('9', clf9)], voting='hard', weights=theweights)\n",
    "params = {'2__C': [1.0, 100.0], '6__n_estimators': [20, 100]}\n",
    "grid = GridSearchCV(estimator=ensemble, param_grid=params, cv=5)\n",
    "np.mean(cross_val_score(grid, Xscaled, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With a given list frompredict, it will return the subset of the entries in frompredict that yield\n",
    "# the best prediction with the model predictionmethod.\n",
    "def bestFeatureSet(numericdataframe, topredict, frompredict, predictionmethod,\n",
    "                   dictofunknown={}, num_iterations=100, transform=\"none\"):\n",
    "    bestscore = 0\n",
    "    bestsubset = []\n",
    "    for ii in range(1,len(frompredict)):\n",
    "        allsubsets = findsubsets(frompredict,ii)\n",
    "        allsubsets = [list(subset) for subset in allsubsets]\n",
    "        for subset in allsubsets:\n",
    "            (X,y) = makeDataFromModels(numericdataframe, topredict, subset, dictofunknown)\n",
    "            score = getPredictionScore(X, y, predictionmethod, num_iterations=num_iterations, transform=transform)\n",
    "            if score[0]>bestscore:\n",
    "                bestscore = score[0]\n",
    "                bestsubset = subset\n",
    "    return bestsubset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findsubsets(set,sizeofsubset):\n",
    "    return set(itertools.combinations(set, sizeofsubset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional reduction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brewerColors(listlength=6):\n",
    "    return [\"#1f78b4\",\"#33a02c\", \"#e31a1c\", \"#ff7f00\", \"#6a3d9a\", \"#b15928\"][:min(12,listlength)]\n",
    "\n",
    "# LDA is supervised, PCA is not. LDA tries to separate the classes as much as possible from each other while doing dim red;\n",
    "# PCA tries to separate the data as much as possible from itself (no labels) when doing dim red.\n",
    "def dimensionalReductionLDA(X, y):\n",
    "    clf = LinearDiscriminantAnalysis(n_components=2)\n",
    "    clf.fit(X,y)\n",
    "    trans_X = clf.transform(X)\n",
    "    uniquevalues = list(set(y))\n",
    "    for ii in uniquevalues:\n",
    "        if trans_X.shape[1]==1:\n",
    "            xaxisnumbers = range(len(trans_X[y==ii]))\n",
    "            yaxisnumbers = trans_X[y==ii, 0]\n",
    "        else:\n",
    "            xaxisnumbers = trans_X[y==ii, 0]\n",
    "            yaxisnumbers = trans_X[y==ii, 1]\n",
    "        plt.scatter(xaxisnumbers, yaxisnumbers, alpha=0.3, color=brewerColors()[ii], label=ii)\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESSION MODELS:\n",
    " - when going from quant to quant\n",
    " - if descrete, just round the numbers\n",
    " - don't treat it like a classification: when evaluating errors I should not count number of cases I hit exactly right\n",
    " - I should try all versions of linear regressions, then nonlinear ones\n",
    " - think about resizing data? Maybe that's more useful for feature selection, where the normalized data forces parameters to all be \"worth\" equal and we can assess which parameters are more important.\n",
    "\n",
    "GETTING QUANT FROM QUANT: regression may be best. I sohuld also try the more advanced methods of SVM and random forests\n",
    "GETTING QUANT FROM CATEG/ORD: the more categories the better. Need to look into the classification algorithms, but it may be that I can guess the quantitiative value from the combination of categorical values. Maybe Random Forests can take in a combination of quant and categ\n",
    "GETTING CATEG FROM QUANT: logistic regression could work OK. Otherwise try the other classification algorithms\n",
    "GETTING CATEG FROM CATEG: again, random forests etc. could be a good tactics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Still to do:\n",
    " - Make a function that does clustering\n",
    " - Make a function that does feature engineering\n",
    " - Make plots to evaluate how well a certain prediction did (e.g. confusion matrices and ROC curves)\n",
    " - As helper-functions, might need to do bootstrapping functions etc., unless they're already in-built\n",
    " - Race models agains each other\n",
    " - Check what else we did during S2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
